{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization for medical image segmentation with 2D U-Net on Intel(R) Xeon CPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Brain MRI scan\n",
    "Magnetic resonance imaging (MRI) of the brain is a safe and painless test that uses a magnetic field and radio waves to produce detailed images of the brain and the brain stem. An MRI differs from a CAT scan (also called a CT scan or a computed axial tomography scan) because it does not use radiation.\n",
    "\n",
    "An MRI scanner consists of a large doughnut-shaped magnet that often has a tunnel in the center. Patients are placed on a table that slides into the tunnel. Some centers have open MRI machines that have larger openings and are helpful for patients with claustrophobia. MRI machines are located in hospitals and radiology centers.\n",
    "\n",
    "During the exam, radio waves manipulate the magnetic position of the atoms of the body, which are picked up by a powerful antenna and sent to a computer. The computer performs millions of calculations, resulting in clear, cross-sectional black and white images of the body. These images can be converted into three-dimensional (3-D) pictures of the scanned area. This helps pinpoint problems in the brain and the brain stem when the scan focuses on those areas.\n",
    "\n",
    "**Reference:** https://kidshealth.org/en/parents/mri-brain.html\n",
    "\n",
    "<table><tr><td><img src='https://github.com/IntelAI/unet/raw/master/3D/images/BRATS_152_img3D.gif'></td><td><img src='https://github.com/IntelAI/unet/blob/master/3D/images/BRATS_195_img.gif?raw=true'></td></tr></table>\n",
    "\n",
    "**Reference:** https://github.com/IntelAI/unet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. U-Net for brain segmentation\n",
    "\n",
    "U-Net implementation in TensorFlow for FLAIR abnormality segmentation in brain MRI based on a deep learning segmentation algorithm used in [Association of genomic subtypes of lower-grade gliomas with shape features automatically extracted by a deep learning algorithm](https://doi.org/10.1016/j.compbiomed.2019.05.002).\n",
    "\n",
    "```latex\n",
    "@article{buda2019association,\n",
    "  title={Association of genomic subtypes of lower-grade gliomas with shape features automatically extracted by a deep learning algorithm},\n",
    "  author={Buda, Mateusz and Saha, Ashirbani and Mazurowski, Maciej A},\n",
    "  journal={Computers in Biology and Medicine},\n",
    "  volume={109},\n",
    "  year={2019},\n",
    "  publisher={Elsevier},\n",
    "  doi={10.1016/j.compbiomed.2019.05.002}\n",
    "}\n",
    "```\n",
    "\n",
    "Topology structured as the following:\n",
    "<img src='https://github.com/mateuszbuda/brain-segmentation-pytorch/raw/master/assets/unet.png'>\n",
    "\n",
    "**Reference:** https://github.com/mateuszbuda/brain-segmentation-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Dataset\n",
    "We use [brain tumor segmentation (BraTS) subset](https://drive.google.com/file/d/1A2IU8Sgea1h3fYLpYtFb2v7NYdMjvEhU/view?usp=sharing) of the [Medical Segmentation Decathlon](http://medicaldecathlon.com/) dataset. The dataset has the [Creative Commons Attribution-ShareAlike 4.0 International license](https://creativecommons.org/licenses/by-sa/4.0/).\n",
    "\n",
    "Please follow instructions [here](https://github.com/IntelAI/unet/blob/master/2D/00_Prepare-Data.ipynb) to prepare the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Let's do coding!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras as K\n",
    "import h5py\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from data import load_data\n",
    "from model import unet\n",
    "\n",
    "import sys; sys.argv=['']; del sys\n",
    "from argparser import args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Check TensorFlow version, and do sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are using Tensorflow version 1.15.2 with Intel(R) oneDNN enabled\n"
     ]
    }
   ],
   "source": [
    "print (\"We are using Tensorflow version\", tf.__version__,\\\n",
    "       \"with Intel(R) oneDNN\", \"enabled\" if tf.pywrap_tensorflow.IsMklEnabled() else \"disabled\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Define the DICE coefficient and loss function\n",
    "The Sørensen–Dice coefficient is a statistic used for comparing the similarity of two samples. Given two sets, X and Y, it is defined as\n",
    "\n",
    "\\begin{equation}\n",
    "Dice = \\frac{2|X\\cap Y|}{|X|+|Y|}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_dice(target, prediction, smooth=0.01):\n",
    "    \"\"\"\n",
    "    Sorensen Dice coefficient\n",
    "    \"\"\"\n",
    "    prediction = np.round(prediction)\n",
    "\n",
    "    numerator = 2.0 * np.sum(target * prediction) + smooth\n",
    "    denominator = np.sum(target) + np.sum(prediction) + smooth\n",
    "    coef = numerator / denominator\n",
    "\n",
    "    return coef\n",
    "\n",
    "def calc_soft_dice(target, prediction, smooth=0.01):\n",
    "    \"\"\"\n",
    "    Sorensen (Soft) Dice coefficient - Don't round preictions\n",
    "    \"\"\"\n",
    "    numerator = 2.0 * np.sum(target * prediction) + smooth\n",
    "    denominator = np.sum(target) + np.sum(prediction) + smooth\n",
    "    coef = numerator / denominator\n",
    "\n",
    "    return coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5 Load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training image dimensions:   (58464, 144, 144, 4)\n",
      "Training mask dimensions:    (58464, 144, 144, 1)\n",
      "Validation image dimensions: (4608, 144, 144, 4)\n",
      "Validation mask dimensions:  (4608, 144, 144, 1)\n",
      "Testing image dimensions: (6624, 144, 144, 4)\n",
      "Testing mask dimensions:  (6624, 144, 144, 1)\n",
      "Number of imgs_warmup: 500\n",
      "Number of imgs_infere: 2000\n"
     ]
    }
   ],
   "source": [
    "data_path = os.path.join(\"../../data/decathlon/144x144/\")\n",
    "data_filename = \"Task01_BrainTumour.h5\"\n",
    "hdf5_filename = os.path.join(data_path, data_filename)\n",
    "imgs_train, msks_train, imgs_validation, msks_validation, imgs_testing, msks_testing = load_data(hdf5_filename)\n",
    "imgs_warmup=imgs_testing[:500]\n",
    "imgs_infere=imgs_testing[500:2500]\n",
    "print(\"Number of imgs_warmup: {}\".format(imgs_warmup.shape[0]))\n",
    "print(\"Number of imgs_infere: {}\".format(imgs_infere.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5 Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data format = channels_last\n",
      "WARNING:tensorflow:From /mnt/sdb/jingxu1/framework/tensorflow/venvs/venv_intel_pip_py37/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /mnt/sdb/jingxu1/framework/tensorflow/venvs/venv_intel_pip_py37/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:OMP_NUM_THREADS is no longer used by the default Keras config. To configure the number of threads, use tf.config.threading APIs.\n",
      "WARNING:tensorflow:From /mnt/sdb/jingxu1/framework/tensorflow/exps/medical-decathlon/2D/model.py:151: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /mnt/sdb/jingxu1/framework/tensorflow/venvs/venv_intel_pip_py37/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unet_model = unet()\n",
    "model = unet_model.load_model(os.path.join(\"./output/unet_model_for_decathlon.hdf5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Define function to inference on input images and plot results out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(model, imgs_validation, msks_validation, img_no):\n",
    "    img = imgs_validation[[img_no], ]\n",
    "    msk = msks_validation[[img_no], ]\n",
    "    \n",
    "    pred_mask = model.predict(img, verbose=1, steps=None)\n",
    "\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(img[0, :, :, 0], cmap=\"bone\", origin=\"lower\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"MRI Input\", fontsize=20)\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(msk[0, :, :, 0], origin=\"lower\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Ground truth\", fontsize=20)\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(pred_mask[0, :, :, 0], origin=\"lower\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Prediction\\nDice = {:.4f}\".format(calc_dice(pred_mask, msk)), fontsize=20)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6 Run inference and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = h5py.File(\"../../data/decathlon/144x144/Task01_BrainTumour.h5\", \"r\")\n",
    "imgs_testing = df[\"imgs_validation\"]\n",
    "msks_testing = df[\"msks_validation\"]\n",
    "files_testing = df[\"testing_input_files\"]\n",
    "indicies_validation = [40,61,400,1100,4385]\n",
    "for idx in indicies_validation:\n",
    "    plot_results(model, imgs_validation, msks_validation, idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def do_benchmark(batch_size=32):\n",
    "    if 'OMP_NUM_THREADS' in os.environ:\n",
    "        print(os.environ['OMP_NUM_THREADS'])\n",
    "    model.predict(imgs_warmup, batch_size, verbose=1, steps=None)\n",
    "    t0 = time.time()\n",
    "    model.predict(imgs_infere, batch_size, verbose=1, steps=None)\n",
    "    t1 = time.time()\n",
    "    lat=(t1-t0)/imgs_infere.shape[0]\n",
    "    thr=1/lat\n",
    "    print('Latency: {}ms; Throughput: {}fps'.format(lat*1000, thr*batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default configuration\n",
      "500/500 [==============================] - 8s 15ms/step\n",
      " 807/2000 [===========>..................] - ETA: 18s"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-38242fb459b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"default configuration\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0munsetEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mdo_benchmark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"customized configuration\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0msetEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-79-37cbf6b1ad75>\u001b[0m in \u001b[0;36mdo_benchmark\u001b[0;34m(batch_size)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs_warmup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs_infere\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mlat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mimgs_infere\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/framework/tensorflow/venvs/venv_intel_pip_py37/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1460\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1461\u001b[0m                                             \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1462\u001b[0;31m                                             callbacks=callbacks)\n\u001b[0m\u001b[1;32m   1463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/framework/tensorflow/venvs/venv_intel_pip_py37/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mbatch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'predict'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'begin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/framework/tensorflow/venvs/venv_intel_pip_py37/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/framework/tensorflow/venvs/venv_intel_pip_py37/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def setEnv():\n",
    "    os.environ['OMP_NUM_THREADS']='24'\n",
    "    os.environ[\"KMP_BLOCKTIME\"]='0'\n",
    "    os.environ[\"KMP_AFFINITY\"]=\"granularity=core,compact,1,0\"\n",
    "    os.environ[\"KMP_SETTINGS\"]=\"0\"\n",
    "\n",
    "    os.environ[\"INTRA_THREADS\"]='24'\n",
    "    os.environ[\"INTER_THREADS\"]='1'\n",
    "\n",
    "def unsetEnv():\n",
    "    if 'OMP_NUM_THREADS' in os.environ:\n",
    "        del os.environ['OMP_NUM_THREADS']\n",
    "    if 'KMP_BLOCKTIME' in os.environ:\n",
    "        del os.environ[\"KMP_BLOCKTIME\"]\n",
    "    if 'KMP_AFFINITY' in os.environ:\n",
    "        del os.environ[\"KMP_AFFINITY\"]\n",
    "    if 'KMP_SETTINGS' in os.environ:\n",
    "        del os.environ[\"KMP_SETTINGS\"]\n",
    "\n",
    "    if 'INTRA_THREADS' in os.environ:\n",
    "        del os.environ[\"INTRA_THREADS\"]\n",
    "    if 'INTER_THREADS' in os.environ:\n",
    "        del os.environ[\"INTER_THREADS\"]\n",
    "\n",
    "batch_size = 1\n",
    "print(\"default configuration\")\n",
    "unsetEnv()\n",
    "do_benchmark(batch_size)\n",
    "print(\"customized configuration\")\n",
    "setEnv()\n",
    "do_benchmark(batch_size)\n",
    "unsetEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    print(os.environ['TEST'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "def setEnv():\n",
    "    os.environ['TEST']='1'\n",
    "setEnv()\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "   http://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\n",
    "SPDX-License-Identifier: EPL-2.0\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
