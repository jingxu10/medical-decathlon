{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization for Medical Image Segmentation with 2D U-Net on Intel(R) Xeon Scalable Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agenda\n",
    "\n",
    "1. Background information introduction\n",
    "2. Intel's optimization technologies on Intel(R) Xeon Scalable Processors\n",
    "3. Let's do coding!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Background Information Introduction\n",
    "#### 1.1 Brain MRI scan\n",
    "Magnetic resonance imaging (MRI) of the brain is a safe and painless test that uses a magnetic field and radio waves to produce detailed images of the brain and the brain stem. An MRI differs from a CAT scan (also called a CT scan or a computed axial tomography scan) because it does not use radiation.\n",
    "\n",
    "An MRI scanner consists of a large doughnut-shaped magnet that often has a tunnel in the center. Patients are placed on a table that slides into the tunnel. Some centers have open MRI machines that have larger openings and are helpful for patients with claustrophobia. MRI machines are located in hospitals and radiology centers.\n",
    "\n",
    "During the exam, radio waves manipulate the magnetic position of the atoms of the body, which are picked up by a powerful antenna and sent to a computer. The computer performs millions of calculations, resulting in clear, cross-sectional black and white images of the body. These images can be converted into three-dimensional (3-D) pictures of the scanned area. This helps pinpoint problems in the brain and the brain stem when the scan focuses on those areas.\n",
    "\n",
    "**Reference:** https://kidshealth.org/en/parents/mri-brain.html\n",
    "\n",
    "<table><tr><td><img src='https://github.com/IntelAI/unet/raw/master/3D/images/BRATS_152_img3D.gif'></td><td><img src='https://github.com/IntelAI/unet/blob/master/3D/images/BRATS_195_img.gif?raw=true'></td></tr></table>\n",
    "\n",
    "**Reference:** https://github.com/IntelAI/unet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 U-Net for brain images segmentation\n",
    "\n",
    "U-Net implementation in TensorFlow for FLAIR abnormality segmentation in brain MRI based on a deep learning segmentation algorithm used in [Association of genomic subtypes of lower-grade gliomas with shape features automatically extracted by a deep learning algorithm](https://doi.org/10.1016/j.compbiomed.2019.05.002).\n",
    "\n",
    "```latex\n",
    "@article{buda2019association,\n",
    "  title={Association of genomic subtypes of lower-grade gliomas with shape features automatically extracted by a deep learning algorithm},\n",
    "  author={Buda, Mateusz and Saha, Ashirbani and Mazurowski, Maciej A},\n",
    "  journal={Computers in Biology and Medicine},\n",
    "  volume={109},\n",
    "  year={2019},\n",
    "  publisher={Elsevier},\n",
    "  doi={10.1016/j.compbiomed.2019.05.002}\n",
    "}\n",
    "```\n",
    "\n",
    "Topology structured as the following:\n",
    "<img src='https://github.com/mateuszbuda/brain-segmentation-pytorch/raw/master/assets/unet.png'>\n",
    "\n",
    "**Reference:** https://github.com/mateuszbuda/brain-segmentation-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Intel's optimization technologies on Intel(R) Xeon Scalable Processors\n",
    "\n",
    "In order to take full advantage of Intel® architecture and to extract maximum performance, the TensorFlow framework has been optimized with Intel® Math Kernel Library for Deep Neural Networks (Intel® oneDNN) primitives, a popular performance library for deep learning applications.\n",
    "\n",
    "For more information about the optimizations as well as performance data, see the blog post:[TensorFlow Optimizations on Modern Intel® Architecture](https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture).\n",
    "\n",
    "Installation guide of Intel Optimization for TensorFlow can be found at [Intel® Optimization for TensorFlow Installation Guide](https://software.intel.com/en-us/articles/intel-optimization-for-tensorflow-installation-guide).\n",
    "\n",
    "\n",
    "#### 2.1 Optimization with TensorFlow switches\n",
    "**intra_op_parallelism_threads**\n",
    "- Number of threads in each threadpool for an operation (like matrix multiplication or reduction). \n",
    "- Recommend: #physical cores, found in Linux with ‘lscpu’ command. \n",
    "\n",
    "**inter_op_parallelism_threads**\n",
    "- Number of thread pools for independent operations.\n",
    "- Recommend: #cpu sockets,  found in Linux with ‘lscpu’ command.\n",
    "\n",
    "Note, need to test with the model & platform to find the best parameters.\n",
    "\n",
    "#### 2.2 Optimization with Intel(R) oneDNN switches\n",
    "Intel oneDNN utilizes OpenMP to leverage Intel architecture.\n",
    "Following environment variables for vectorization and multi-threading.\n",
    "\n",
    "**KMP_AFFINITY**\n",
    "- Restricts execution of certain threads to a subset of the physical processing units in a multiprocessor computer.\n",
    "- Recommend: ```export KMP_AFFINITY=granularity=fine,compact,1,0```\n",
    "\n",
    "**KMP_BLOCKTIME**\n",
    "- Set the time (milliseconds), that a thread wait for, after completing the execution of a parallel region, before sleeping. \n",
    "- Recommend: ```export KMP_BLOCKTIME=0 (or 1)```\n",
    "\n",
    "**OMP_NUM_THREADS**\n",
    "- Set maximum number of threads to use for OpenMP parallel regions\n",
    "- Recommend: ```export OMP_NUM_THREADS=num physical cores```\n",
    "\n",
    "Note, recommend users tuning these values for their specific neural network model and platform.\n",
    "\n",
    "#### 2.3 Optimization with miscellaneous configurations/tools\n",
    "**Numactl**\n",
    "- Running on a NUMA-enabled machine brings with it special considerations. NUMA or non-uniform memory access is a memory layout design used in data center machines meant to take advantage of locality of memory in multi-socket machines with multiple memory controllers and blocks. In most cases, inference runs best when confining both the execution and memory usage to a single NUMA node.\n",
    "- Recommend: ```numactl --cpunodebind=N --membind=N python <pytorch_script>```\n",
    "\n",
    "**Batch size**\n",
    "- Can increase usage and efficiency of hardware resources.\n",
    "- Optional according to your requirements.\n",
    "\n",
    "A more detailed introduction of maximizing performance with Intel Optimization for TensorFlow can be found [here](https://software.intel.com/en-us/articles/maximize-tensorflow-performance-on-cpu-considerations-and-recommendations-for-inference).\n",
    "\n",
    "https://software.intel.com/en-us/articles/maximize-tensorflow-performance-on-cpu-considerations-and-recommendations-for-inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Let's do coding!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.0 Dataset\n",
    "We use [brain tumor segmentation (BraTS) subset](https://drive.google.com/file/d/1A2IU8Sgea1h3fYLpYtFb2v7NYdMjvEhU/view?usp=sharing) of the [Medical Segmentation Decathlon](http://medicaldecathlon.com/) dataset. The dataset has the [Creative Commons Attribution-ShareAlike 4.0 International license](https://creativecommons.org/licenses/by-sa/4.0/).\n",
    "\n",
    "Please follow instructions [here](https://github.com/IntelAI/unet/blob/master/2D/00_Prepare-Data.ipynb) to prepare the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras as K\n",
    "import h5py\n",
    "import math\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import sys; sys.argv=['']; del sys\n",
    "\n",
    "from argparser import args\n",
    "from data import load_data\n",
    "from model import unet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Check TensorFlow version, and do sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"We are using Tensorflow version\", tf.__version__,\\\n",
    "       \"with Intel(R) oneDNN\", \"enabled\" if tf.pywrap_tensorflow.IsMklEnabled() else \"disabled\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Define the DICE coefficient and loss function\n",
    "The Sørensen–Dice coefficient is a statistic used for comparing the similarity of two samples. Given two sets, X and Y, it is defined as\n",
    "\n",
    "\\begin{equation}\n",
    "Dice = \\frac{2|X\\cap Y|}{|X|+|Y|}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_dice(target, prediction, smooth=0.01):\n",
    "    \"\"\"\n",
    "    Sorensen Dice coefficient\n",
    "    \"\"\"\n",
    "    prediction = np.round(prediction)\n",
    "\n",
    "    numerator = 2.0 * np.sum(target * prediction) + smooth\n",
    "    denominator = np.sum(target) + np.sum(prediction) + smooth\n",
    "    coef = numerator / denominator\n",
    "\n",
    "    return coef\n",
    "\n",
    "def calc_soft_dice(target, prediction, smooth=0.01):\n",
    "    \"\"\"\n",
    "    Sorensen (Soft) Dice coefficient - Don't round preictions\n",
    "    \"\"\"\n",
    "    numerator = 2.0 * np.sum(target * prediction) + smooth\n",
    "    denominator = np.sum(target) + np.sum(prediction) + smooth\n",
    "    coef = numerator / denominator\n",
    "\n",
    "    return coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(\"../../data/decathlon/144x144/\")\n",
    "data_filename = \"Task01_BrainTumour.h5\"\n",
    "hdf5_filename = os.path.join(data_path, data_filename)\n",
    "imgs_train, msks_train, imgs_validation, msks_validation, imgs_testing, msks_testing = load_data(hdf5_filename)\n",
    "imgs_warmup=imgs_testing[:500]\n",
    "imgs_infere=imgs_testing[500:2500]\n",
    "print(\"Number of imgs_warmup: {}\".format(imgs_warmup.shape[0]))\n",
    "print(\"Number of imgs_infere: {}\".format(imgs_infere.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_model = unet()\n",
    "model = unet_model.load_model(os.path.join(\"./output/unet_model_for_decathlon.hdf5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6 Define function to inference on input images and plot results out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(model, imgs_validation, msks_validation, img_no):\n",
    "    img = imgs_validation[idx:idx+1]\n",
    "    msk = msks_validation[idx:idx+1]\n",
    "    \n",
    "    pred_mask = model.predict(img, verbose=1, steps=None)\n",
    "\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(img[0, :, :, 0], cmap=\"bone\", origin=\"lower\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"MRI Input\", fontsize=20)\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(msk[0, :, :, 0], origin=\"lower\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Ground truth\", fontsize=20)\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(pred_mask[0, :, :, 0], origin=\"lower\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Prediction\\nDice = {:.4f}\".format(calc_dice(pred_mask, msk)), fontsize=20)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.7 Run inference and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicies_validation = [40, 63, 43, 55, 99]\n",
    "for idx in indicies_validation:\n",
    "    plot_results(model, imgs_validation, msks_validation, idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.8 Demo of 2D U-Net inference optimization\n",
    "\n",
    "See demo in console.\n",
    "\n",
    "*Performance results:*\n",
    "- Latency\n",
    "  - Time spent for processing 1 image.\n",
    "  - Unit: millisecond per frame (ms/f)\n",
    "- Throughput\n",
    "  - Number of processed images in 1 second.\n",
    "  - Unit: frames per second (f/s)\n",
    "\n",
    "*Runs:*\n",
    "- Single stream\n",
    "  - Batch size 1\n",
    "    - Without numactl\n",
    "      - Default configuration\n",
    "      - Configuration with optimization\n",
    "    - With numactl\n",
    "      - Default configuration\n",
    "      - Configuration with optimization\n",
    "  - Batch size 128\n",
    "    - With numactl\n",
    "      - Default configuration\n",
    "      - Configuration with optimization\n",
    "- Multiple streams\n",
    "  - Batch size 128\n",
    "    - Configuration with optimization\n",
    "      - 2 streams\n",
    "      - 4 streams\n",
    "      - 8 streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.8.1 Configuration with optimization & numactl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_perf(x_labels, latencys, throughputs, xlabel, batch_size):\n",
    "    x = np.arange(len(x_labels))\n",
    "    plt.figure(figsize=(16,6))\n",
    "    plt.subplot(121)\n",
    "    ax1 = plt.gca()\n",
    "    ax1.bar(x, latencys, width=0.7)\n",
    "    ax1.set_title('Latency (batch size: {}, the less the better)'.format(batch_size))\n",
    "    ax1.set_xlabel(xlabel)\n",
    "    ax1.set_ylabel('Latency (ms)')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(x_labels)\n",
    "    for rect, label in zip(ax1.patches, latencys):\n",
    "        height = rect.get_height()\n",
    "        ax1.text(rect.get_x() + rect.get_width() / 2, height, label, ha='center', va='bottom')\n",
    "\n",
    "    plt.subplot(122)\n",
    "    ax2 = plt.gca()\n",
    "    ax2.bar(x, throughputs, color='tab:green', width=0.7)\n",
    "    ax2.set_title('Throughput (batch size: {}, the larger the better)'.format(batch_size))\n",
    "    ax2.set_xlabel(xlabel)\n",
    "    ax2.set_ylabel('Through (fps)')\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(x_labels)\n",
    "    for rect, label in zip(ax2.patches, throughputs):\n",
    "        height = rect.get_height()\n",
    "        ax2.text(rect.get_x() + rect.get_width() / 2, height, label, ha='center', va='bottom')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load result performance data\n",
    "rsts = []\n",
    "with open('04_rst_opt_numa.csv', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        rst = line.strip().split(',')\n",
    "        rsts.append(rst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size: 1\n",
    "x_labels = ['default conf', 'conf with optmization', 'conf with opt + numactl']\n",
    "latencys_conf_1 = [float(rsts[1][2]), float(rsts[2][2]), float(rsts[3][2])]\n",
    "throughputs_conf_1 = [float(rsts[1][3]), float(rsts[2][3]), float(rsts[3][3])]\n",
    "plot_perf(x_labels, latencys_conf_1, throughputs_conf_1, 'Configuration', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size: 128\n",
    "x_labels = ['default conf', 'conf with optmization', 'conf with opt + numactl']\n",
    "latencys_conf_128 = [float(rsts[4][2]), float(rsts[5][2]), float(rsts[6][2])]\n",
    "throughputs_conf_128 = [float(rsts[4][3]), float(rsts[5][3]), float(rsts[6][3])]\n",
    "plot_perf(x_labels, latencys_conf_128, throughputs_conf_128, 'Configuration', 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.8.2 number of streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load result performance data\n",
    "rsts = []\n",
    "with open('04_rst_instances.csv', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        rst = line.strip().split(',')\n",
    "        rsts.append(rst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size: 1\n",
    "x_labels = [1, 2, 4, 8]\n",
    "latencys_stream_1 = [float(rsts[1][2]), float(rsts[3][2]), float(rsts[4][2]), float(rsts[5][2])]\n",
    "throughputs_stream_1 = [float(rsts[1][3]), float(rsts[3][3]), float(rsts[4][3]), float(rsts[5][3])]\n",
    "plot_perf(x_labels, latencys_stream_1, throughputs_stream_1, 'Number of streams', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size: 128\n",
    "x_labels = [1, 2, 4, 8]\n",
    "latencys_stream_128 = [float(rsts[2][2]), float(rsts[6][2]), float(rsts[7][2]), float(rsts[8][2])]\n",
    "throughputs_stream_128 = [float(rsts[2][3]), float(rsts[6][3]), float(rsts[7][3]), float(rsts[8][3])]\n",
    "plot_perf(x_labels, latencys_stream_128, throughputs_stream_128, 'Number of streams', 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_summary(x_labels, y_labels, xlabel, ylabel, title):\n",
    "    x = np.arange(len(x_labels))\n",
    "    plt.figure(figsize=(16,6))\n",
    "    ax = plt.gca()\n",
    "    ax.bar(x, y_labels, width=0.7)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(x_labels)\n",
    "    for rect, label in zip(ax.patches, y_labels):\n",
    "        height = rect.get_height()\n",
    "        ax.text(rect.get_x() + rect.get_width() / 2, height, label, ha='center', va='bottom')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.0 Scenarios:\n",
    "1. You need to get the inference result as soon as possible. (Online processing)\n",
    "2. You need to process a large bunch of data in a certain period. (Offline processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Online processing\n",
    "\n",
    "From **latency** perspective point of view.\n",
    "\n",
    "\\* Run 1 stream processing 1 image/frame on 1 socket with configuration with optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_labels = ['default conf,\\nbs=1, stream=1', 'conf with optmization,\\nbs=1, stream=1', 'conf with opt + numactl,\\nbs=1, stream=1', 'conf with opt + numactl,\\nbs=1, stream=2', 'conf with opt + numactl,\\nbs=1, stream=4']\n",
    "x = np.arange(len(x_labels))\n",
    "latencys_summary = [latencys_conf_1[0], latencys_conf_1[1], latencys_conf_1[2], latencys_stream_1[1], latencys_stream_1[2]]\n",
    "plot_summary(x_labels, latencys_summary, 'Configuration', 'Latency (ms)', 'Latency with different configurations (the less the better)')\n",
    "\n",
    "print('Number of sockets: 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Offline processing\n",
    "\n",
    "From **throughput** perspective point of view.\n",
    "\n",
    "\\* Run multiple streams processing a batch of images/frames on all sockets with configuration with optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_labels = ['conf with opt + numactl,\\nbs=1, stream=1', 'conf with opt + numactl,\\nbs=128, stream=1', 'conf with opt + numactl,\\nbs=128, stream=4', 'conf with opt + numactl,\\nbs=128, stream=8']\n",
    "x = np.arange(len(x_labels))\n",
    "throughputs_summary = [throughputs_conf_1[2], throughputs_conf_128[2], throughputs_stream_128[1], throughputs_stream_128[2]]\n",
    "plot_summary(x_labels, throughputs_summary, 'Configuration', 'Throughput (fps)', 'Throughput with different configurations (the larger the better)')\n",
    "\n",
    "print('Number of sockets: 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The optimization methods in this course are not the best.**\n",
    "\n",
    "**All optimization methods should be adjusted based on test result. It depends on the detailed model, dataset, HardWare & user case.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "   http://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\n",
    "SPDX-License-Identifier: EPL-2.0\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
